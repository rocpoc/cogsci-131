{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bias_and_variance import print_equation, format_equation\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-info\">**Hint**: Much of the material covered in this problem is introduced in the Geman et al. (1992) reading. If you are having trouble with the conceptual questions, this might be a good place to look.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "We wish to evaluate the performance of a learning algorithm that takes a set of $(x,y)$ pairs as input and selects a function $g(x)$, which predicts the value of $y$ for a given $x$ (that is, for a given $(x,y)$ pair, $g(x)$ should approximate $y$).\n",
    "\n",
    "We will evaluate the 'fit' of the functions that our learning algorithm selects using the *mean squared error* (MSE) between $g(x)$ and $y$. For a set of $n$ data points $\\{ (x_1, y_1), \\ldots, (x_n,y_n) \\}$, the MSE associated with a function $g$ is calculated as\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n \\left ( y_i - g(x_i) \\right )^2 .\n",
    "$$\n",
    "\n",
    "The set of candidate functions that we will allow our algorithm to consider is the set of $k$th-order polynomials. This means that our hypothesis space will contain all functions of the form $g(x) = p_kx^k + p_{k-1} x^{k-1} + \\ldots + p_{1} x + p_0$. These functions are entirely characterized by their coefficient vector ${\\bf p} = (p_k, p_{k-1}, \\ldots, p_0)$, and become much more flexible as $k$ increases (think about the difference between linear ($k=1$), quadratic ($k=2$), and cubic ($k=3$) functions). If we are given a set of $(x,y)$ pairs, it is straightforward to find the $k$th-order polynomial that minimizes the MSE between $g(x)$ and the observed $y$ values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-info\">For those who have done some statistics, the calculation for finding ${\\bf p}$ is just a case of linear regression where the various powers of $x$ are the predictors</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "In this problem, we'll be trying to learn the function $g(x)$ that generated some data with the addition of some Gaussian (i.e., normally distributed) noise. The data is a $110\\times 2$ array, where the first column corresponds to the $x$ coordinate, and the second column corresponds to the $y$ coordinate (which is the function evaluated at the corresponding $x$ value, i.e. $y = g(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05326841,  0.11153722],\n",
       "       [ 0.09595921,  0.08886825],\n",
       "       [ 0.33218686,  0.04323005],\n",
       "       [ 0.36718942,  0.05728251],\n",
       "       [ 0.52694255,  0.03424077],\n",
       "       [ 0.56136812,  0.05958364],\n",
       "       [ 0.71448407,  0.13404962],\n",
       "       [ 0.69834226,  0.10344032],\n",
       "       [ 0.96030273,  0.19107459],\n",
       "       [ 0.88821028,  0.1567958 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# only show the first ten points, since there are a lot\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "You will use this data to train and evaluate your learning algorithm.\n",
    "  \n",
    "A \"learning algorithm\" which finds a polynomial of given degree that minimizes the MSE on a set of $(x,y)$ coordinates is implemented in Python with the `np.polyfit` command. You can use `p = np.polyfit(x, y, k)` to return the coefficient vector ${\\bf p}$ for the $k$th-order polynomial $g$ that best fits (i.e., has the smallest $MSE$ on) the $x$ and $y$ coordinates in `x` and `y`.\n",
    "\n",
    "For example, to fit a 4th-order polynomial to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector of coefficients: [ 0.22138257 -0.45087857  0.75871103 -0.43306212  0.12531311]\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$g(x)=0.22x^{4}-0.45x^{3}+0.76x^{2}-0.43x+0.13$$"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit the 4th order polynomial\n",
    "p = np.polyfit(data[:, 0], data[:, 1], 4)\n",
    "print(\"Vector of coefficients: \" + str(p))\n",
    "\n",
    "# display the resulting equation\n",
    "print_equation(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "You can calculate the values of $g$ at a set of $x$ coordinates using the command `np.polyval`. For example, if you wanted to compute $g(x)$ at $x=0$, $x=0.5$, and $x=1$, you could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12531311,  0.0559364 ,  0.22146603])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.polyval(p, np.array([0, 0.5, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part A (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Run the following cell to call `make_polynomial_fit_and_graph`, which creates an IPython *widget* that will allow you to explore what happens when you try to fit polynomials of different orders to different subsets of the data. You should read the source code to see how this is accomplished.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "819d0263f55eb9fc3bec63416edf4881",
     "grade": false,
     "grade_id": "make_polynomial_fit_and_graph",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d645c99931478b800d093c7722b0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='polynomial_order', max=9), IntSlider(value=6, description='training_set_index', max=11, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "@interact\n",
    "def make_polynomial_fit_and_graph(polynomial_order=(0, 9), training_set_index=(1, 11)):\n",
    "    \"\"\"Finds the best-fitting polynomials for k = {0, ... , 9}, \n",
    "    using one of eleven different training datasets.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # relabel the parameters\n",
    "    k = polynomial_order\n",
    "    i = training_set_index\n",
    "    \n",
    "    # pull out the x and y values\n",
    "    x = data[((i - 1) * 10):(i * 10), 0]\n",
    "    y = data[((i - 1) * 10):(i * 10), 1]\n",
    "    \n",
    "    # create the figure\n",
    "    fig, axis = plt.subplots()\n",
    "\n",
    "    # create a range of values for x between 0 and 1\n",
    "    plotx = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "    # find the coefficients p\n",
    "    p = np.polyfit(x, y, k)\n",
    "\n",
    "    # find the values of the polynomial parameterized by p and \n",
    "    # evaluated for the points plotx\n",
    "    ploty = np.polyval(p, plotx)\n",
    "\n",
    "    # plot the fitted function\n",
    "    axis.plot(plotx, ploty, 'b-', label=\"${}$\".format(format_equation(p)))\n",
    "\n",
    "    # plot the original data points\n",
    "    axis.plot(x, y, 'ko')\n",
    "    \n",
    "    # set the axis limits\n",
    "    axis.set_xlim(0, 1)\n",
    "    axis.set_ylim(0, 0.35)\n",
    "\n",
    "    # put a title on each plot\n",
    "    axis.set_title('Dataset #{} fitted with k = {}'.format(i, k))\n",
    "    \n",
    "    # create a legend\n",
    "    axis.legend(loc='upper left', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Examine the figure that is produced. Try changing the widget sliders to change the dataset that we are fitting the polynomial to, and the degree of that polynomial. Which degree polynomial both results in similar fits (i.e. similar coefficients) across all datasets _and_ does a good job at capturing the data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8744ea4fdcc8267df64fcb6fe0bc4c7e",
     "grade": true,
     "grade_id": "part_a",
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "After trying different polynomials across the different datasets, I believe a 4th degree polynomial results in similar fits and does a good job at capturing the data wihtout overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">How can the above result be understood in terms of the bias and variance tradeoff we discussed in class?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b76a983a3ecfd4bdfeb301ab0f0e781d",
     "grade": true,
     "grade_id": "part_a2",
     "locked": false,
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "I believe the 4th degree polynomial has a good tradeoff of bias and variance here. If we increase the polynomial degree, we end up low bias and high variance which then ends up overfitting the data. However if we decrease the degree—say to 1 or 2— we end up with high bias low variance which tends to underfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part B (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "To get a more quantitative sense of how well each polynomial order fits the data, we'll now compute the actual mean squared error (MSE) of the polynomial fits in relationship to both a *training* dataset and a *testing* dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Complete the function `mse` to compute the MSE for a polynomial with order $k$ that has been fitted to the training data. The completed `mse` function should return a tuple containing the MSE values for the training data and the test data.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d03138435844e846f5c3b254b35ca85f",
     "grade": false,
     "grade_id": "mse",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mse(k, train, test):\n",
    "    \"\"\"Fits a polynomial with order `k` to a training dataset, and \n",
    "    then returns the mean squared error (MSE) between the y-values\n",
    "    of the training data and the fitted polynomial, and the MSE\n",
    "    between the y-values of the test data and the fitted polynomial.\n",
    "    \n",
    "    Your answer can be done in 6 lines of code, including the return\n",
    "    statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k : integer\n",
    "        The polynomial order\n",
    "    train : numpy array with shape (n, 2)\n",
    "        The training data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "    test : numpy array with shape (m, 2)\n",
    "        The testing data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a 2-tuple consisting of the training set MSE and testing set MSE\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE   \n",
    "\n",
    "    p = np.polyfit(train[:, 0], train[:, 1], k)\n",
    "    train_g = np.polyval(p, train[:, 0])\n",
    "    test_g = np.polyval(p, test[:, 0])\n",
    "    \n",
    "    train_list = []\n",
    "    n = 0\n",
    "    for i in train:\n",
    "        train_list.append((i[1] - train_g[n])**2)\n",
    "        n = n+1\n",
    "    train_list = (1/len(train_g)) * np.sum(train_list)\n",
    "    \n",
    "    test_list = []\n",
    "    j = 0\n",
    "    for i in test:\n",
    "        test_list.append((i[1] - test_g[j])**2)\n",
    "        j = j+1\n",
    "    test_list = (1/len(test_g)) * np.sum(test_list)\n",
    "    \n",
    "    return (train_list, test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "For example, we can compute the MSE for $k=2$ by using the first ten datapoints as training data, and the other datapoints as testing data, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.00022974495516730833\n",
      "The testing error is:  0.0003742983713362151\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# compute the MSE\n",
    "train_mse, test_mse = mse(2, data[:10], data[10:])\n",
    "print(\"The training error is: {}\".format(train_mse))\n",
    "print(\"The testing error is:  {}\".format(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c669b3ae0b7f0d7b6534d5add4706ea7",
     "grade": true,
     "grade_id": "test_mse",
     "points": 1
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test that the `mse` function is correct.\"\"\"\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# use first ten, and the remaining\n",
    "assert_allclose(mse(2, data[:10], data[10:]), (0.000229744955167, 0.000374298371336))\n",
    "assert_allclose(mse(3, data[:10], data[10:]), (0.000169612346303, 0.000463251756094))\n",
    "assert_allclose(mse(9, data[:10], data[10:]), (1.46448764925e-21, 0.337001581723), atol=1e-20)\n",
    "\n",
    "# use half-and-half\n",
    "assert_allclose(mse(2, data[:55], data[55:]), (0.00034502281024316553, 0.00037620706341530435))\n",
    "assert_allclose(mse(3, data[:55], data[55:]), (0.0003378190977339938, 0.00039980736728858482))\n",
    "assert_allclose(mse(9, data[:55], data[55:]), (0.00026755111091101571, 0.00061531514687572487))\n",
    "\n",
    "# use last twenty, and the remaining\n",
    "assert_allclose(mse(2, data[-20:], data[:-20]), (0.00030881029910697136, 0.00040876086505745344))\n",
    "assert_allclose(mse(3, data[-20:], data[:-20]), (0.00021713262385879197, 0.00055653317636801015))\n",
    "assert_allclose(mse(9, data[-20:], data[:-20]), (0.00012210662449207329, 0.00071987940235435685))\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part C (1 point)\n",
    "\n",
    "Next, complete the function template `plot_mse` to plot MSE versus $k$ for both `traindata` and `testdata`. Be sure to include a proper legend, title, and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "81d706e0d731700e60f45803cfc5df80",
     "grade": false,
     "grade_id": "plot_mse",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_mse(axis, max_order, train, test):\n",
    "    \"\"\"Plot the mean squared error (MSE) for the given training and testing\n",
    "    data as a function of polynomial order. \n",
    "    \n",
    "    * Your plot should show the MSE for 0 <= k < max_order\n",
    "    * There should be two lines: one black, for the training set error, and\n",
    "      one red, for the testing set error.\n",
    "    * Make sure to include labels for the x- and y- axes.\n",
    "    * Label the training error and testing error lines as \"Training set error\" \n",
    "      and \"Testing set error\", respectively. These labels will be used to\n",
    "      create a legend later on (and so you should NOT actually create the\n",
    "      legend yourself -- just label the lines).\n",
    "      \n",
    "    Your answer can be done in 10 lines of code, including the return statement.\n",
    "      \n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : matplotlib axis object\n",
    "        The axis on which to plot the MSE\n",
    "    max_order : integer\n",
    "        The maximum polynomial order to compute a fit for\n",
    "    train : numpy array with shape (n, 2)\n",
    "        The training data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "    test : numpy array with shape (m, 2)\n",
    "        The testing data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array with shape (max_order, 2)\n",
    "        The MSE for the training data (corresponding to the first column) and\n",
    "        for the testing data (corresponding to the second column). Each row\n",
    "        corresponds to a different polynomial order.\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    axis.set_xlabel(\"K degree polynomial\")\n",
    "    axis.set_ylabel(\"Mean Squared Error\")\n",
    "    axis.set_title(\"Plot MSE\")\n",
    "    \n",
    "    # plot train data\n",
    "    axis.plot([x for x in range(max_order)], \n",
    "              [mse(x, train, test)[0] for x in range(max_order)], color=\"k\", label = \"Training set error\")\n",
    "    \n",
    "    ## plot test data\n",
    "    axis.plot([x for x in range(max_order)], \n",
    "              [mse(x, train, test)[1] for x in range(max_order)], color=\"r\", label = \"Testing set error\")\n",
    "    \n",
    "    mse_arr = np.empty((max_order, 2))\n",
    "    n = 0\n",
    "    while n < max_order:\n",
    "        mse_arr[n][0] = mse(n, train, test)[0]\n",
    "        mse_arr[n][1] = mse(n, train, test)[1]\n",
    "        n = n+1\n",
    "    return mse_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "After implementing the `plot_mse` function, you should be able to see the error as a function of the polynomial order for both the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f35cc3ace10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35cc424710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# plot it\n",
    "fig, axis = plt.subplots()\n",
    "plot_mse(axis, 15, data[:20], data[20:])\n",
    "axis.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d03c9e73819596e818ba71ef24e9d347",
     "grade": true,
     "grade_id": "test_plot_mse",
     "points": 1
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Is the plot_mse function correctly implemented?\"\"\"\n",
    "from nose.tools import assert_equal, assert_not_equal\n",
    "from numpy.testing import assert_allclose\n",
    "from plotchecker import get_data\n",
    "\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# check that it uses the mse function\n",
    "old_mse = mse\n",
    "del mse\n",
    "try:\n",
    "    fig, axis = plt.subplots()\n",
    "    plot_mse(axis, 9, data[:10], data[10:])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"plot_mse should call mse, but it does not\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    mse = old_mse\n",
    "    del old_mse\n",
    "    \n",
    "fig, axis = plt.subplots()\n",
    "error = plot_mse(axis, 9, data[:10], data[10:])\n",
    "axis.legend(loc='upper left')\n",
    "\n",
    "# check the error\n",
    "assert_equal(error.shape, (9, 2))\n",
    "assert_allclose(error[0], mse(0, data[:10], data[10:]))\n",
    "assert_allclose(error[4], mse(4, data[:10], data[10:]))\n",
    "assert_allclose(error[8], mse(8, data[:10], data[10:]))\n",
    "\n",
    "# check the plotted data\n",
    "plotted_data = get_data(axis)\n",
    "assert_equal(plotted_data.shape, (18, 2))\n",
    "assert_allclose(plotted_data[:9, 0], np.arange(9))\n",
    "assert_allclose(plotted_data[9:, 0], np.arange(9))\n",
    "assert_allclose(plotted_data[:9, 1], error[:, 0])\n",
    "assert_allclose(plotted_data[9:, 1], error[:, 1])\n",
    "\n",
    "# check the line colors\n",
    "assert axis.lines[0].get_color() in ['k', 'black', (0, 0, 0), '#000000']\n",
    "assert axis.lines[1].get_color() in ['r', 'red', (1, 0, 0), '#FF0000']\n",
    "\n",
    "# check the legend\n",
    "legend_labels = [x.get_text() for x in axis.get_legend().get_texts()]\n",
    "assert_equal(legend_labels, [\"Training set error\", \"Testing set error\"])\n",
    "\n",
    "# check the axis labels\n",
    "assert_not_equal(axis.get_xlabel(), \"\")\n",
    "assert_not_equal(axis.get_ylabel(), \"\")\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part D (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Now, we will use another IPython widget to visualize how the error changes depending on the dataset that we are fitting to. The widget will call your `plot_mse` function with different subsets of the data, depending on the index that is set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f2aabe0affa0b962a31a80ef1883e568",
     "grade": false,
     "grade_id": "visualize_mse",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58423524b77648c58d8da8a986a47a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=6, description='training_set_index', max=11, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "@interact\n",
    "def visualize_mse(training_set_index=(1, 11)):\n",
    "    # relabel the index for convenience\n",
    "    i = training_set_index\n",
    "    \n",
    "    # pull out the training and testing data\n",
    "    traindata = data[((i - 1) * 10):(i * 10)]\n",
    "    testdata = np.concatenate([data[:((i - 1) * 10)], data[(i * 10):]])\n",
    "\n",
    "    # plot the MSE\n",
    "    fig, axis = plt.subplots()\n",
    "    plot_mse(axis, 10, traindata, testdata)\n",
    "    axis.set_ylim(0, 0.01)\n",
    "    axis.set_title(\"MSE for dataset #{}\".format(i))\n",
    "    axis.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Try changing the widget slider to explore the error on different datasets. What happens to the MSE on the training sets as the degree of the polynomial (i.e., the size of our hypothesis space) increases? What about on the test sets?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "561bd622b8514f51ab0176f6e1db36b5",
     "grade": true,
     "grade_id": "part_d",
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "As the degree of the polynomial increases, the MSE on the training sets decreases until a certain point, then typically starts to increase in error again. However on some of the different subsets of the data, the point where the error starts increasing again is different. For instance after subset 5 and consecutive ones, the error skyrockets at much smaller degrees of the polynomial (4, 5, and 6) instead of the common 8 that is a trend in most of the other subsets. On degree 4, the error does not skyrocket at k=8, and typically for the early subsets (1-4) the error decreases from k = 0 to k = 2, and then somewhat balances out until k = 6, then increases again.\n",
    "\n",
    "On the test sets, there is a much more straightforward trend of decreasing the error as the degree of the polynomial increases, across all different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Should you always use a learning algorithm that gives the best results on the training set? Justify your answer.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0f85252250c580bf29fff0d3b7674a83",
     "grade": true,
     "grade_id": "part_d2",
     "locked": false,
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "Not necessarily. As we can see, just because the algorithm produces lower error in training as the degree of the polynomial increases, this may result in the testing set overfitting data because it has too high variance and low bias, which consequently leads the testing set to have higher error rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part E (1 point)\n",
    "  \n",
    "The MSE on the test set is an approximation of the *prediction error* associated with a particular learning algorithm. In class we discussed how *expected prediction error* is composed of a bias and a variance term. Both the bias and the variance depend upon the true function, $f$, that we are estimating with our learning algorithm.\n",
    "\n",
    "* An algorithm with high *bias* will systematically produce predictions that differ from the true function. This can happen in situations where the true function is more complex than the most complex function available to the algorithm.\n",
    "* An algorithm with high *variance* will produce predictions that can vary wildly depending on the specifics of the dataset we are evaluating. This can happen in situations where the algorithm is able to return functions that are more complex than the true function.\n",
    "\n",
    "For a more detailed explanation of the bias-variance tradeoff, please take a look at the reading by Geman et al. (1992)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Explain how the results from the MSE on `testset` could be explained in terms of the bias and variance of the learning algorithms involved. That is, in which cases does the bias of the learning algorithm dominate the MSE? In which cases does the variance dominate?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b15b28b14cdcdf465beb81a96a0c77a6",
     "grade": true,
     "grade_id": "part_e",
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "As we can see in the graphs, it is a trend that increasing the complexity will always decrease the training error. The testing set error also initially falls, however once a certain point of complexity is reached, the error for the testing set starts to rise. This is because at when our model uses high levels of complexity, it can in fact very accurately fit the training data, but this in itself causes the model to perform worse at predicting new data in the testing set, which is called overfitting. So after this \"sweet spot\" of minimum error for a bias/variance tradeoff, we are overfitting our model, and before this \"sweet spot\" we are therefore underfitting our model. \n",
    "\n",
    "We know that low bias/high variance corresponds to overfitting, so after the sweet spot/high levels of degree of the polynomial we see a low bias and high variance and therfore the variance dominates. \n",
    "Furthermore, high bias / low variance corresponds to underfitting, so before the sweet spot/low levels of degree of the polynomial we see high bias and low variance, and therefore the bias dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Explain how bias and variance are related to the degree of the polynomial in our model.</div>\n",
    "<!--<div class=\"alert alert-success\">Explain *why* either the bias or variance matters more in the different cases disussed above.</div>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8e347daa05ec97253664a2c7bbbb5da9",
     "grade": true,
     "grade_id": "part_e2",
     "locked": false,
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "Explained above, but again in our model, variance is increased while bias is decreased in relation to the degree of the polynomial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors!\n"
     ]
    }
   ],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "20fb91264d73476293d305a36c9d5057": {
     "views": [
      {
       "cell_index": 33
      }
     ]
    },
    "48b11bcc67b548028ca74ebd12e5a75b": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
